I"8<p align="justify">In this guide, I'll be explaining a couple of more advanced methods of reinforcement learning; namely Advantage Actor Critic (A2C) and Proximal Policy Optimization (PPO), but first a recap of actor critic methods.</p>

<h3 id="recap-of-actor-critic">Recap of Actor Critic</h3>

<p>Recall computing the policy gradient</p>

\[\begin{align*}
\nabla_{\theta}J(\theta) &amp;= \mathbb{E}_{\tau \sim \pi_{\theta}}[\nabla_{\theta}\log{\pi_{\theta}(\tau)}R(\tau)] \\
&amp;= \Big(\sum_{t=0}^{T - 1}\nabla_{\theta}\log{\pi_{\theta}(a_{t}|s_{t})}\Big) \cdot \sum_{t=0}^{T - 1} r_{t}
\end{align*}\]

<p align="justify">This works fine, but for a trajectory $\tau = (s_{0}, a_{0}, ..., s_{T-1}, a_{T-1})$, we might not get a reward signal until the very end, making it incredible difficult to determine which actions in that trajectory were actually influential in leading to that final (hopefully good) reward. Essentially, vanilla policy gradients is very sample inefficient. One way we can deal with this, is by replacing the $R(\tau)$ term with something that incorporates both state and action pairs at each time step. The Q value does this!</p>

\[Q(s_{t}, a_{t}) = \mathbb{E}_{s_{t}, a_{t}, ..., s_{T-1}, a_{T-1}}\Big[\sum_{t'=t}^{T-1}r_{t'}\Big]\]

<p align="justify">Recall that $Q(s, a)$ is the expected future reward of taking action $a$ in state $s$. Of course we can also discount the reward by $\gamma$.</p>

\[\mathbb{E}_{s_{t}, a_{t}, ..., s_{T-1}, a_{T-1}}\Big[\sum_{t'=t}^{T-1}\gamma^{t' - t}r_{t'}\Big]\]

<p align="justify">There's just one issue; how do we actually calculate this Q value? Well we can use a neural network parameterized by some $w$ to estimate the Q values. This network is known as the "critic" in actor critic. We call this form of actor critic "Q actor critic".</p>

<center>
  <div class="col-lg-12 col-md-12 col-sm-12 col-xs-12">
    <figure class="figure">
        <img src="/assets/A2C-PPO/actor-critic.png" />
        <figcaption class="figure-caption text-center">Image Credit: Chris Yoon's "Understanding Actor Critic Methods and A2C" medium post</figcaption>
      </figure>   
  </div>
</center>

<h3 id="advantage-actor-critic">Advantage Actor Critic</h3>

<p align="justify">
Q actor critic still suffers from high variance during training in practice, causing slow convergence. One thing we can do to reduce this variance is to define the advantage function.</p>

\[A(s_{t}, a_{t}) = Q(s_{t}, a_{t}) - V(s_{t})\]

<p align="justify">One intuitive way of thinking about why this decreases the variance is the following: The Q function tells us the expected future reward of being in state $s_{t}$ and taking action $a_{t}$, following some policy $\pi$. The value function tells us the expected future reward of being in state $s_{t}$ and following policy $\pi$. So in a sense, we're taking $Q(s_{t}, a_{t})$, which has high variance, and subtracting the value of $s_{t}$. The policy $\pi$ might tell us that action $a^{1}$ is best to take in $s_{t}$. Some actions are better, some actions are worse. Consequentially, some Q values are better and some are worse, depending on the action. But by subtracting $V(s_{t})$, we get advantage values $A(s_{t}, a_{t})$ that are closer together. Recall the Bellman equation.</p>

\[Q(s_{t}, a_{t}) = r_{t} + \gamma\mathbb{E}[V(s_{t+1})]\]

<p align="justify">We can use this to substitute $Q(s_{t}, a_{t})$ in the advantage calculation.</p>

\[\begin{align*}
A(s_{t}, s_{t}) &amp;= Q(s_{t}, a_{t}) - V(s_{t}) \\
&amp;= r_{t} + \gamma V(s_{t+1}) - V(s_{t})
\end{align*}\]

<p align="justify">Now instead of using a neural network to estimate the Q function, we use it to estimate the value function $V(\cdot)$.</p>

<h3 id="proximal-policy-optimization">Proximal Policy Optimization</h3>

<p align="justify">Proximal Policy Optimization (PPO) improve further upon Advantage Actor Critic (A2C).</p>

:ET