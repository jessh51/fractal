<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/fractal/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/fractal/" rel="alternate" type="text/html" /><updated>2020-02-09T22:19:05-08:00</updated><id>http://localhost:4000/fractal/feed.xml</id><title type="html">Fractal</title><subtitle>Fractal is an organization that provides tutorials for topics related to artifical intelligence. We are also involved with educational outreach related to AI such as giving talks at local schools.</subtitle><entry><title type="html">Deep Q-Learning</title><link href="http://localhost:4000/fractal/dl/2020/02/05/deep-q-learning.html" rel="alternate" type="text/html" title="Deep Q-Learning" /><published>2020-02-05T00:06:43-08:00</published><updated>2020-02-05T00:06:43-08:00</updated><id>http://localhost:4000/fractal/dl/2020/02/05/deep-q-learning</id><content type="html" xml:base="http://localhost:4000/fractal/dl/2020/02/05/deep-q-learning.html">&lt;h3 id=&quot;learning-based-methods&quot;&gt;Learning-Based Methods&lt;/h3&gt;

&lt;p&gt;Policy and Value Iteration gave us a solid way to find the optimal policy when we have perfect information about the environment (i.e. we know the distributions of state transitions and rewards), but when this information is not know, we have to get clever with how we determine good policies. One way is to learn by trial and error - taking actions in the environment and observing what states we transition to under different actions and what rewards we obtain for doing so. Doing this gives us data in the form $(s, a, r, s’)$. If we take action $a$ in state $s$ we receive reward $r$ and transition to state $s’$. From this data we can try to approximate the unknown distributions.&lt;/p&gt;

&lt;p&gt;Another issue we face is large state spaces. Policy and Value iteration worked fine for Gridworld (small state space), but when the total number of states becomes large, these algorithms become intractable - they contain a $|\mathbf{S}|^{3}$ and a $|\mathbf{S}|^{2}$ term respectively in their time complexities! Our solution to this issue is to learn a lower-dimensional representation of the state using neural networks. This is known as deep reinforcement learning, and the type we will be exploring in this guide is called deep Q-Learning.&lt;/p&gt;

&lt;h3 id=&quot;deep-q-learning&quot;&gt;Deep Q-Learning&lt;/h3&gt;

&lt;p&gt;Essentially what we are trying to do is approximate $Q^{\ast}(s, a)$ using a neural network. If we can get a good approximation of $Q^{\ast}$, we can extract a good policy. This neural network will be parameterized by a generic term $\theta$, will take as input the state $s$ and output value for each possible action, which we can perform a max operation over to get the best action to take.&lt;/p&gt;

&lt;p&gt;In order to learn such a function, we need to define a loss function so that our network knows what it’s optimizing for. Recall that the optimal Q function satisfies&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q^{\ast}(s,a) = \mathbb{E}_{s' \sim p(s'|s, a)}\bigg[r(s, a) + \gamma \max_{a'}Q^{\ast}(s', a')\bigg]&lt;/script&gt;

&lt;p&gt;Assume we have a bunch of data in the form ${(s, a, r, s’)}_{i=1}^{N}$. Then for one of the data points, we can measure how close our Q network approximates the optimal Q function by the following equation.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{MSE Loss} = (Q_{t}(s, a) - [r + \max_{a} Q_{t-1}(s', a)])^{2}&lt;/script&gt;

&lt;p&gt;Where $Q_{t}$ and $Q_{t-1}$ represent the network output before and after a single weight update. Notice how the first term in the square is our network’s current output and the second term is the target Q-value that we want, but based on the old network weights. The training pipeline looks something like below. First we college a batch of data (i.e. the agent takes actions in the environment) of size $B$, then we feed that data into the network, compute the loss, and update our network weights. Below, $D$ is the dimensionality of the state representation (e.g. the number of pixels in an image).&lt;/p&gt;

&lt;center&gt;
  &lt;div class=&quot;col-lg-10 col-md-10 col-sm-12 col-xs-12&quot;&gt;
    &lt;img src=&quot;/fractal/assets/Deep_Q_Learning/q-network-training.png&quot; /&gt;
  &lt;/div&gt;
&lt;/center&gt;

&lt;h3 id=&quot;epsilon-greedy-and-experience-replay&quot;&gt;Epsilon Greedy and Experience Replay&lt;/h3&gt;

&lt;p&gt;This framework gives us a good way to approximate the optimal Q function, but there still remains the question of how do we actually collect the data? What policy should we use for that? To better explain this problem let’s consider an example. Say we have some sub-optimal policy $\pi_{0}$ that we will use to collect experience $(s, a, r, s’)$ data in the environment. If we simply choose the best action for each state according to this sub-optimal policy, we may not discover that some actions that are not chosen by $\pi_{0}$ lead to good rewards. Essentially, we will be stuck in local minima. One way around this is to occasionally take random actions so that we have a chance of seeing new experiences and hopefully finding better actions to take. This is an exploration strategy known as epsilon-greedy. It says that for some time $t$ the action we choose should be made according to the following rule.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
a_{t} =
\begin{cases}
\arg\max_{a}Q_{t}(s, a) &amp; \text{with probability } 1 - \epsilon \\
\text{random action} &amp; \text{with probability } \epsilon
\end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;This will allow our agent to do some exploring to find good state-action combinations. Typically, it is good to do a lot of exploration when the network first starts training by using a high value for epsilon, reducing epsilon gradually as training progresses.&lt;/p&gt;

&lt;p&gt;The next issue we run into is that consecutive data is highly correlated, which can lead to feedback loops or just really slow training. For example, if we are gathering data under a policy that tells the agent to move down, then data that represents this type of action will be overrepresented in the next iteration of training even though the better option might be to go right and we just haven’t explored that yet. To solve this, one solution is to maintain a buffer that stores data $(s, a, r, s’)$ that we continually update while the agent moves through the environment, removing old data as the buffer gets full. When it comes time to sample a batch of data for training, we randomly sample from this buffer rather that take a bunch of consecutive data like before. This approach is called experience replay.&lt;/p&gt;

&lt;p&gt;Armed with the knowledge of these common problems and some solid ways to address them, we present the full Deep Q-Learning algorithm with Experience Replay.&lt;/p&gt;

&lt;center&gt;
  &lt;figure&gt;
    &lt;div class=&quot;col-lg-12 col-md-12 col-sm-12 col-xs-12&quot;&gt;
      &lt;img src=&quot;/fractal/assets/Deep_Q_Learning/DQN-algorithm.png&quot; /&gt;
      &lt;figcaption&gt;Credit: Fei-Fei Li, Justin Johnson, Serena Yeung: CS231n&lt;/figcaption&gt;
    &lt;/div&gt;
  &lt;/figure&gt;
&lt;/center&gt;

&lt;p&gt;The function $\phi$ is just a preprocessing step before inputting the data into the neural network and can be ignored for our purposes. The curious reader can explore the full paper from DeepMind: &lt;a href=&quot;https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf&quot;&gt;Playing Atari with Deep Reinforcement Learning&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We have seen a method for approximating Q function using neural networks by gathering experience data from within the environment and using it to train the network as well as some problems that arise from this approach. We have also seen reasonable ways to deal with these problems. Next, we will learn methods for estimating the optimal policy without going through the middle-man of estimating a Q function.&lt;/p&gt;</content><author><name></name></author><summary type="html">Learning-Based Methods</summary></entry><entry><title type="html">Reinforcement Learning Background</title><link href="http://localhost:4000/fractal/dl/2020/02/02/reinforcement-learning.html" rel="alternate" type="text/html" title="Reinforcement Learning Background" /><published>2020-02-02T00:06:43-08:00</published><updated>2020-02-02T00:06:43-08:00</updated><id>http://localhost:4000/fractal/dl/2020/02/02/reinforcement-learning</id><content type="html" xml:base="http://localhost:4000/fractal/dl/2020/02/02/reinforcement-learning.html">&lt;h3 id=&quot;reinforcement-learning&quot;&gt;Reinforcement Learning&lt;/h3&gt;

&lt;p&gt;Reinforcement learning (RL) is difference from supervised and unsupervised learning. In supervised learning, we have truth data (labels) for our problem that we use to check the output of our model against, correcting for mistakes accordingly. In unsupervised learning, we are learning some structure to the data. In RL we don’t have data necessarily, but instead we have an environment and a set of rules. There exists an agent that lives in this environment and its objective is to take actions that will eventually lead to reward. Whereas supervised learning tries to match data to its corresponding label, in RL we try to maximize reward. In other words, we are learning how to make the agent make a good sequence of actions.&lt;/p&gt;

&lt;center&gt;
  &lt;div class=&quot;col-lg-6 col-md-6 col-sm-12 col-xs-12&quot;&gt;
    &lt;img src=&quot;/fractal/assets/RL_Intro/rl-schema.png&quot; /&gt;  
  &lt;/div&gt;
&lt;/center&gt;

&lt;h3 id=&quot;framing-an-rl-problem&quot;&gt;Framing an RL Problem&lt;/h3&gt;

&lt;p&gt;We well frame an RL problem as a Markov Decision Process (MDP), which is a fancy-sounding way of formulating decision making under uncertainty. We will define the following ideas that will guide us in formulating the problem:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathbf{S}$: The set of possible states&lt;/li&gt;
  &lt;li&gt;$\mathbf{A}$: The set of possible actions the agent can take&lt;/li&gt;
  &lt;li&gt;$R(s, a, s’)$: A probability distribution of the reward given for being in state $s$, taking action $a$ and ending up in a new state $s’$&lt;/li&gt;
  &lt;li&gt;$\mathbb{T}(s, a, s’)$: A probability distribution of state transitions&lt;/li&gt;
  &lt;li&gt;$\gamma \in [0, 1)$: A scalar discount factor (will come in handy later)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some literature will also use $\mathbf{O}$ which is the set of possible observations given to the agent by the environment. This is sometimes the same as $\mathbf{S}$ and sometimes not. In a fully observable MDP, the agent has has all information about the state of the environment, so when the agent receives an observation $o_{i} \in \mathbf{O}$, it contains the same information as the state of the environment $s_{i} \in \mathbf{S}$. An example of this is chess - each player (agent) knows exactly what the state of the game is at any time. In a partially observable MDP this is not the case. The agent does not have access to the full state of the environment, so when it received an observation, it does not contain the same information as the state of the environment, hence these are two difference concepts. An example of this is poker - each player does not know the cards of the other players and therefore does not have access to the full state of the game.&lt;/p&gt;

&lt;p&gt;The last concept is a policy, which is a function $\pi(s) : \mathbf{S} \Rightarrow \mathbf{A}$ that tells us which action to take given a state. The whole idea of RL is to learn a good policy; one that will tell us good actions to take in each state of the environment. A policy can interpreted deterministically $\pi(s)$ (The action taken when we are in state $s$), or stochastically $\pi(a|s)$ (the probability of taking action $a$ in state $s$).&lt;/p&gt;

&lt;p&gt;Most of the time in RL, we do not have access to the true distributions $R(s, a, s’)$ and $\mathbb{T}(s, a, s’)$. If we had these distributions, we could easily calculate the optimal policy, however without this information we have to estimate them by trying out actions in our environment and seeing if we get reward or not.&lt;/p&gt;

&lt;h3 id=&quot;grid-world&quot;&gt;Grid World&lt;/h3&gt;

&lt;p&gt;For now, we will assume we have access to the distributions $R(s, a, s’)$ and $\mathbb{T}(s, a, s’)$ so that we can really drive home the point that if we have the true distributions at hand, we can calculate the optimal policy. Image we have the following problem.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The agent lives in a grid, where each square is a state. This is the state space.&lt;/li&gt;
  &lt;li&gt;The agent can move North, South, East, or West (N, S, E, W). This is the action space.&lt;/li&gt;
  &lt;li&gt;80% of the time, the action the agent takes does as it is intended. 10% of the time the agent slips and moves to one side, and 10% of the time the agent slips to the other side. For example if the agent chooses to move north, there is a 80% chance it will do so, a 10% chance it will move west, and a 10% chance it will move east. This is the transition probability distribution.&lt;/li&gt;
  &lt;li&gt;There is a destination state that deterministically gives the agent a reward of +1 for reaching it and a terminal state that deterministically gives the agent a reward of -1 for reaching it.&lt;/li&gt;
&lt;/ul&gt;

&lt;center&gt;
  &lt;div class=&quot;col-lg-8 col-md-8 col-sm-12 col-xs-12&quot;&gt;
    &lt;img src=&quot;/fractal/assets/RL_Intro/gridworld-example.png&quot; /&gt;  
  &lt;/div&gt;
&lt;/center&gt;

&lt;h3 id=&quot;finding-optimal-policies&quot;&gt;Finding Optimal Policies&lt;/h3&gt;

&lt;p&gt;So now that we have a concrete example of a problem, we can discuss what it means to find an optimal policy for it. Some questions that come when determining what a “good” policy is are “does it maximize the reward right now?” and “does is maximize the future reward?”. Typically, we maximize the discounted future reward; the idea being that we want policies that take future state into consideration, but we also don’t want the policy to focus so much on optimizing for future rewards that it doesn’t take actions that would put the agent in a good state now. Therefore we define the optimal policy $\pi^{\ast}$ in the following way.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi^{\ast} = \arg \max_{\pi} \mathbb{E}\bigg[\sum_{t \geq 0} \gamma^{t}r_{t}|\pi\bigg]&lt;/script&gt;

&lt;p&gt;Here, time is indexed by $t$. This means we want to maximize the expectation of the discounted reward given some policy. Notice since $\gamma$ is between 0 and 1, we will optimize for states closer in time more than ones further.&lt;/p&gt;

&lt;h4 id=&quot;value-function-and-q-function&quot;&gt;Value Function and Q-Function&lt;/h4&gt;

&lt;p&gt;We have a notion of what a “good” policy is, but how do we actually find it? This is where the Value function and Q function come in. The value function is a prediction of future reward and basically answers the question “how good is the current state $s$ that I’m in?”. We denote $V^{\pi}(s)$ as the expected cumulative reward of being in state $s$ and then following policy $\pi$ thereafter.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V^{\pi}(s) = \mathbb{E}\bigg[\sum_{t \geq 0} \gamma^{t}r_{t}|s_{0}=s, \pi\bigg]&lt;/script&gt;

&lt;p&gt;We also have the notion of an optimal value function $V^{\ast}(s)$, which is the expected cumulative reward of being in state $s$ and then following the optimal policy $\pi^{\ast}$ thereafter. The Q function represents a similar idea - $Q^{\pi}(s, a)$ is the expected cumulative reward for taking action $a$ in state $s$ and then following policy $\pi$ thereafter. Similarly $Q^{\ast}(s, a)$ is the expected cumulative reward of taking action $a$ in state $s$ and following the optimal policy thereafter.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q^{\pi}(s, a) = \mathbb{E}\bigg[\sum_{t \geq 0} \gamma^{t}r_{t}|s_{0}=s, a_{0}=a, \pi\bigg]&lt;/script&gt;

&lt;p&gt;Remember, the value function only deals with states, and the Q function deals with state-action pairs! Now we can go about defining the optimal value and policy from the Q function values. It is clear that the optimal value and policy for a state can be defined in terms of the Q function as follows.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V^{\ast}(s) = \max_{a}Q^{\ast}(s, a)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi^{\ast}(s) = \arg \max_{a}Q^{\ast}(s, a)&lt;/script&gt;

&lt;p&gt;These optimal values can be calculated recursively using what are called the Bellman equations, defined below. Notice how the calculation of these values requires we have access to the true distributions $\mathbb{T}(s’, a, s)$ (denoted with $p(\cdot)$ below) and $R(s’, a, s)$ (denoted with $r(\cdot)$ below).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;V^{\ast}(s) = \max_{a}\sum_{s'}p(s'|s, a)[r(s, a) + \gamma V^{\ast}(s')]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Q^{\ast}(s, a) = \sum_{s'}p(s'|s, a)[r(s, a) + \gamma V^{\ast}(s')]&lt;/script&gt;

&lt;p&gt;The summation over all possible next-states $s’$ of $p(s’|s, a)$ comes from the definition of expectation in probability $\mathbb{E}[f(\cdot)] = \sum_{x}p(x) \cdot f(x)$. We are summing over all subsequent states the probability of being in that state, given the current state and action, then multiplying by the reward we get for being in that next state. It should be clear that the expected reward of being in state $s$, taking action $a$ and ending up in state $s’$ is exactly $r(s, a) + \gamma V^{\ast}(s’)$.&lt;/p&gt;

&lt;p&gt;To reiterate, if we know the distributions $\mathbb{T}$ and $R$, we have a recursive way of calculating the optimal Q value of any state-action pair, and hence we can extract the optimal policy. Now we will go over two algorithms for doing so.&lt;/p&gt;

&lt;h3 id=&quot;value-iteration&quot;&gt;Value Iteration&lt;/h3&gt;

&lt;p&gt;The idea of value iteration pretty much exactly follows the logic we described above. The algorithm is as follows.&lt;/p&gt;

&lt;center&gt;
  &lt;div class=&quot;col-lg-10 col-md-10 col-sm-12 col-xs-12&quot;&gt;
    &lt;img src=&quot;/fractal/assets/RL_Intro/VI-algorithm.png&quot; /&gt;  
  &lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;Each iteration of Value Iteration costs $O(|\mathbf{S}|^{2}|\mathbf{A}|)$ time and is very expensive for large state spaces. Recall our grid world game with values for each state initialized to 0.&lt;/p&gt;

&lt;center&gt;
  &lt;div class=&quot;col-lg-6 col-md-6 col-sm-12 col-xs-12&quot;&gt;
    &lt;img src=&quot;/fractal/assets/RL_Intro/gridworld-VI-step1.png&quot; /&gt;  
  &lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;Let’s do an example calculation of one iteration of Value Iteration on the state (3, 3) (where the agent is pictured).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*}
V^{2}((3, 3)) &amp;= \max_{a}\sum_{s'}p(s'|(3, 3), a)[r(s, (3, 3)) + \gamma V^{1}(s')] \\
&amp;= \sum_{s'\in \{(4, 3), (3, 2)\}} p(s'|(3, 3), \text{right})[r((3, 3), \text{right}) + \gamma V^{1}(s')] \\
&amp;= (0.8 * (0 + \gamma * 1)) + (0.1 (0 + \gamma * 0)) + (0.1 (0 + \gamma * 0)) \\
&amp;= 0.8\gamma
\end{align*} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that the above calculation did not include other actions for brevity since we already knew the max operation would give us right as the optimal action. Now state (3, 3) has value $0.8\gamma$ and we can keep recursing to calculate the values of all the other states. After doing so, this would be 1 iteration of Value Iteration. We would repeat this process until the values converge.&lt;/p&gt;

&lt;h3 id=&quot;policy-iteration&quot;&gt;Policy Iteration&lt;/h3&gt;

&lt;p&gt;The next algorithm we will discuss is called Policy Iteration. The idea is that we start with some policy $\pi_{0}$ and iteratively refine it until the policy does not change anymore (i.e. it has converged). The algorithm involves two steps: computing the value of a policy, then using those values to greedily change the actions chosen by the previous policy to create a new policy.&lt;/p&gt;

&lt;center&gt;
  &lt;div class=&quot;col-lg-10 col-md-10 col-sm-12 col-xs-12&quot;&gt;
    &lt;img src=&quot;/fractal/assets/RL_Intro/PI-algorithm.png&quot; /&gt;  
  &lt;/div&gt;
&lt;/center&gt;

&lt;p&gt;Policy Iteration has time complexity $O(|\mathbf{S}|^{3})$ for each iteration because of the linear system of equations, but in practice it often converges faster than Value Iteration because the policy becomes locked in place faster than the values in Value Iteration.&lt;/p&gt;

&lt;p&gt;Next time we will discuss how to find good policies even when the distributions $\mathbb{T}$ and $R$ are not known. This will largely amount to taking exploratory actions in the environment to collect data about what sequences of actions give good rewards and what sequences don’t. This opens up the door to the field of RL which we will soon begin exploring.&lt;/p&gt;</content><author><name></name></author><summary type="html">Reinforcement Learning</summary></entry><entry><title type="html">Support Vector Machines</title><link href="http://localhost:4000/fractal/ml/2020/01/26/SVMs-and-kernels.html" rel="alternate" type="text/html" title="Support Vector Machines" /><published>2020-01-26T00:06:43-08:00</published><updated>2020-01-26T00:06:43-08:00</updated><id>http://localhost:4000/fractal/ml/2020/01/26/SVMs-and-kernels</id><content type="html" xml:base="http://localhost:4000/fractal/ml/2020/01/26/SVMs-and-kernels.html">&lt;p&gt;$\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$&lt;/p&gt;

&lt;h3 id=&quot;test&quot;&gt;Test&lt;/h3&gt;</content><author><name></name></author><summary type="html">$\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$</summary></entry><entry><title type="html">Vector Spaces, Norms, and Inner Products</title><link href="http://localhost:4000/fractal/mfml/2020/01/23/vector-spaces-norms-and-inner-products.html" rel="alternate" type="text/html" title="Vector Spaces, Norms, and Inner Products" /><published>2020-01-23T00:06:43-08:00</published><updated>2020-01-23T00:06:43-08:00</updated><id>http://localhost:4000/fractal/mfml/2020/01/23/vector-spaces-norms-and-inner-products</id><content type="html" xml:base="http://localhost:4000/fractal/mfml/2020/01/23/vector-spaces-norms-and-inner-products.html">&lt;p&gt;$\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$&lt;/p&gt;

&lt;h3 id=&quot;vector-spaces&quot;&gt;Vector Spaces&lt;/h3&gt;

&lt;p&gt;We will begin our study of the mathematical foundations of machine learning by considering the idea of a vector space. A vector space $\mathbf{S}$ is a set of elements called vectors that obey the following&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;For $\mathbf{x, y, z} \in \mathbf{S}$:
    &lt;ul&gt;
      &lt;li&gt;$\mathbf{x} + \mathbf{y} = \mathbf{y} + \mathbf{x}$ (commutative)&lt;/li&gt;
      &lt;li&gt;$\mathbf{x} + (\mathbf{y} + \mathbf{z}) = (\mathbf{x} + \mathbf{y}) + \mathbf{z}$ (associative)&lt;/li&gt;
      &lt;li&gt;$\mathbf{x} + 0 = \mathbf{x}$&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Scalar multiplication is distributive and associative&lt;/li&gt;
  &lt;li&gt;$\mathbf{S}$ is closed under scalar multiplication and vector addition. i.e.
$\mathbf{x}, \mathbf{y} \in \mathbf{S} \implies a\mathbf{x} + b\mathbf{y} \in S \quad \forall a, b \in \mathbb{R}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The last bullet is arguably the most important and describes the more descriptive “linear vector space”.&lt;/p&gt;

&lt;p&gt;A couple examples of linear vectors spaces are:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;$\mathbb{R}^{N}$&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x} = \begin{bmatrix}x_{1} \\ \vdots \\ x_{N}\end{bmatrix}&lt;/script&gt;

    &lt;p&gt;Note that the addition of any two vectors in $\mathbb{R}^{N}$ is also a vector in $\mathbb{R}^{N}$.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The set of all polynomials of degree $N$&lt;/p&gt;

    &lt;p&gt;Note that for polynomials $p(x) = \alpha_{N}x^{N} + … + \alpha_{1}x + \alpha_{0}$ and $t(x) = \beta_{N}x^{N} + … + \beta_{1}x + \beta_{0}$, $ap(x) + bt(x)$ is still a polynomial of degree $N$ for any choice of $a$ and $b$, therefore the space of all degree $N$ polynomials is a linear vector space.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Thinking of functions as elements of a vector space might seem strange, but we will soon see that functions can be represented as discrete sets of numbers (i.e. vectors) and manipulated the same way that we normally think about manipulating vectors in $\mathbb{R}^{N}$.&lt;/p&gt;

&lt;h4 id=&quot;linear-subspaces&quot;&gt;Linear Subspaces&lt;/h4&gt;

&lt;p&gt;Now that we have the notion of a vector space, we can introduce the idea of a linear subspace, which is a mathematical tool that will soon become useful. A linear subspace is a subset $\mathbf{T}$ of a vector space $\mathbf{S}$ that contains the zero vector (i.e. $\mathbf{0} \in \mathbf{T}$) and is closed under vector addition and scalar multiplication.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{x}, \mathbf{y} \in \mathbf{T} \implies a\mathbf{x} + b\mathbf{y} \in T \quad \forall a, b \in \mathbb{R}&lt;/script&gt;

&lt;div class=&quot;container&quot;&gt;
  &lt;div class=&quot;row&quot;&gt;
    &lt;div class=&quot;col-lg-6 col-md-6 col-sm-12 col-xs-12&quot;&gt;
      &lt;figure class=&quot;figure&quot;&gt;
        &lt;img src=&quot;/fractal/assets/Vector_Spaces/subspace_counterexample.png&quot; /&gt;
        &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;T is not a linear subspace&lt;/figcaption&gt;
      &lt;/figure&gt;   
    &lt;/div&gt;
    &lt;div class=&quot;col-lg-6 col-md-6 col-sm-12 col-xs-12&quot;&gt;
      &lt;figure class=&quot;figure&quot;&gt;
        &lt;img src=&quot;/fractal/assets/Vector_Spaces/subspace_example.png&quot; /&gt;
        &lt;figcaption class=&quot;figure-caption text-center&quot;&gt;T is a linear subspace&lt;/figcaption&gt;
      &lt;/figure&gt;   
    &lt;/div&gt;
  &lt;/div&gt;
&lt;/div&gt;

&lt;p&gt;In the figure above we see on the left a counter example of a linear subspace. It is a counter example because it does not contain the zero vector and also because it is easy to see we could take a linear combination of two vectors in $\mathbf{T}$ to get a vector outside $\mathbf{T}$, so both conditions are violated. This is not the case for the subspace on the right, and it is in fact a linear subspace of $\mathbf{S} = \mathbb{R}^{2}$.&lt;/p&gt;

&lt;h3 id=&quot;norms&quot;&gt;Norms&lt;/h3&gt;

&lt;p&gt;A Vector space is a set of elements that obey certain properties. By introducing a norm to a particular vector space, we are giving it a sense of distance. A norm $\norm{\cdot}$ is a mapping from a vector space $\mathbf{S}$ to $\mathbb{R}$ such that for all $\mathbf{x, y} \in \mathbf{S}$,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;$\norm{\mathbf{x}} \geq 0$ and $\norm{\mathbf{x}} = 0 \iff \mathbf{x} = \mathbf{0}$&lt;/li&gt;
  &lt;li&gt;$\norm{\mathbf{x} + \mathbf{y}} \leq \norm{\mathbf{x}} + \norm{\mathbf{y}}$ (triangle inequality)&lt;/li&gt;
  &lt;li&gt;$\norm{a\mathbf{x}} = |a|\norm{\mathbf{x}}$ (homogeneity)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;This definition should feel familiar. The norm of a vector $\norm{\mathbf{x}}$ is its distance from the origin and the norm of the difference of two vectors $\norm{\mathbf{x - y}}$ is the distance between the two vectors. Here are some examples of norms that we will be using later on.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The standard euclidean norm (aka the $\ell_{2}$ norm): $\mathbf{S} = \mathbb{R}^{N}$&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\norm{\mathbf{x}}_{2} = \sqrt{\sum_{n=1}^{N}|x_{n}|^{2}}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\mathbf{S} = $ the set of continuous functions on $\mathbb{R}$ ($\mathbf{x}$ is a function)&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\norm{\mathbf{x}}_{2} = \sqrt{\int_{-\infty}^{\infty}|x(t)|^{2}dt}&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;inner-products&quot;&gt;Inner Products&lt;/h3&gt;

&lt;p&gt;By now we have introduced vector spaces and normed vector spaces, the latter being a subset of the former. Now we will introduce the inner product. The inner product $\langle\cdot, \cdot\rangle$ is a function that takes two vectors in a vector space and produces a real number (or complex number, but we will ignore this for now).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle\cdot,\cdot\rangle: \mathbf{S}\times\mathbf{S}\rightarrow \mathbb{R}&lt;/script&gt;

&lt;p&gt;A valid inner product obeys three rules for $\mathbf{x, y, z}\in\mathbf{S}$:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;$\langle\mathbf{x},\mathbf{y}\rangle = \langle\mathbf{y},\mathbf{x}\rangle$ (symmetry)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For $a, b \in \mathbb{R}$&lt;/p&gt;

    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\langle a\mathbf{x} + b\mathbf{y}, \mathbf{z}\rangle = a\langle\mathbf{x}, \mathbf{z}\rangle + b\langle\mathbf{y}, \mathbf{z}\rangle&lt;/script&gt; (linearity)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;$\langle\mathbf{x}, \mathbf{x}\rangle \geq 0$ and $\langle\mathbf{x}, \mathbf{x}\rangle = 0 \iff \mathbf{x} = \mathbf{0}$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Two important examples of inner products are&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The standard inner product (aka the dot product): $\mathbf{S} = \mathbb{R}^{N}$&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle\mathbf{x},\mathbf{y}\rangle = \sum_{n=1}^{N}x_{n}y_{n} = \mathbf{y}^{T}\mathbf{x}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The standard inner product for continuous functions on $\mathbb{R}^{N}$. If $\mathbf{x, y}$ are two such functions&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle\mathbf{x}, \mathbf{y}\rangle = \int_{-\infty}^{\infty}x(t)y(t)dt&lt;/script&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The last concept I want to introduce is the idea of an induced norm. It is a fact that every valid inner product induces a valid norm (but not the other way around). This induces norm has very useful properties that not all other norms have. For some inner product $\langle\cdot,\cdot\rangle_{\mathbf{S}}$ on a vector space $\mathbf{S}$, the induced norm is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\norm{\mathbf{x}}_{\mathbf{S}} = \sqrt{\langle\mathbf{x},\mathbf{x}\rangle_{\mathbf{S}}}&lt;/script&gt;

&lt;p&gt;The standard inner product induces the standard euclidean norm. Two important properties of induced norms (not all norms!) are&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The Cauchy-Schwartz Inequality:&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;|\langle\mathbf{x},\mathbf{y}\rangle| \leq \norm{\mathbf{x}}\norm{\mathbf{y}}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pythagorean Theorem:&lt;/p&gt;

    &lt;p&gt;If $\langle\mathbf{x},\mathbf{y}\rangle = 0$ then $\mathbf{x}$ and $\mathbf{y}$ are orthogonal and $\norm{\mathbf{x} + \mathbf{y}} = \norm{\mathbf{x}} + \norm{\mathbf{y}}$&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;All the ideas presented in these notes are important foundational mathematical concepts that we will make use of in later notes. You should become very familiar with them and know how to determine if an inner product or a norm is valid or not. Now that we have some mathematical tools, next time we will discuss a foundational problem is machine learning - linear approximation.&lt;/p&gt;</content><author><name></name></author><summary type="html">$\newcommand{\norm}[1]{\left\lVert#1\right\rVert}$</summary></entry><entry><title type="html">Machine Learning Introduction</title><link href="http://localhost:4000/fractal/supp/2020/01/15/what-is-ml.html" rel="alternate" type="text/html" title="Machine Learning Introduction" /><published>2020-01-15T00:06:43-08:00</published><updated>2020-01-15T00:06:43-08:00</updated><id>http://localhost:4000/fractal/supp/2020/01/15/what-is-ml</id><content type="html" xml:base="http://localhost:4000/fractal/supp/2020/01/15/what-is-ml.html">&lt;h3 id=&quot;what-is-machine-learning&quot;&gt;What is Machine Learning?&lt;/h3&gt;

&lt;p&gt;Machine Learning (ML) is, as Tom Mitchell stated, “The study of algorithms that improve their performance P at some task T with experience E”. Another way of looking at it is that we are learning an algorithm that solves an inference problem or a model that describes some data set. We will discuss these two concepts at a basic level below and then introduce some cool things machine learning and deep learning have accomplished to hopefully incite some interest.&lt;/p&gt;

&lt;h4 id=&quot;inference&quot;&gt;Inference&lt;/h4&gt;

&lt;p&gt;Inference means making a decision or prediction about some sort of data, perhaps in a probabilistic sense. For example, I might give you an image and say “is there a cat in this image?” or “what is the probability that there is a cat in this image?”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/fractal/assets/MLIntro/kit.jpg&quot; alt=&quot;cat&quot; width=&quot;200&quot; height=&quot;150&quot; /&gt;&lt;img src=&quot;/fractal/assets/MLIntro/cat.jpg&quot; alt=&quot;cat&quot; width=&quot;200&quot; height=&quot;150&quot; /&gt;&lt;img src=&quot;/fractal/assets/MLIntro/catinbox.jpg&quot; alt=&quot;cat&quot; width=&quot;150&quot; height=&quot;150&quot; /&gt;&lt;img src=&quot;/fractal/assets/MLIntro/catbowl.jpg&quot; alt=&quot;cat&quot; width=&quot;150&quot; height=&quot;150&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you might imagine, it gets more complex and ambiguous when the thing you’re trying to predict is secluded in some way or only represents the idea of the thing you’re trying to predict instead of being the thing itself (e.g. the cat-faced bowl above).&lt;/p&gt;

&lt;p&gt;There are perhaps more interesting inference problems where you don’t have complete information, but you know some related information. For example, if I give you the temperature in San Francisco, San Jose, and Fremont, can you predict (infer) the temperature in Palo Alto? Or If I give you the position and velocity of a car at time $t_{1}$, can you tell me the probability the car will be 5 meters north at time $t_{2}$? The output of an inference algorithm can either be a concrete decision (e.g. the image does have a cat in it) or a probability distribution over the set of possible outcomes (e.g. there is a 60% chance that the temperate in Palo Alto is between 50 and 65 degrees Fahrenheit).&lt;/p&gt;

&lt;h4 id=&quot;modeling&quot;&gt;Modeling&lt;/h4&gt;

&lt;p&gt;Modeling allows us to describe data either qualitatively or numerically. There are typically geometric models, which are ones that try to find geometric structure in data and probabilistic models, which try to find a probability distribution given a bunch of samples of random variables.&lt;/p&gt;

&lt;p&gt;An example of a geometric model might be: I have (square-footage, location, number of bedrooms) information for 1000 houses. How can I find some combination of these attributes that still comes close to fitting the data? This boils down to finding a lower-dimensional subspace that comes close to containing all the original data.&lt;/p&gt;

&lt;figure&gt;
  &lt;center&gt;
    &lt;img src=&quot;/fractal/assets/MLIntro/subspace.png&quot; alt=&quot;subspace&quot; width=&quot;300&quot; height=&quot;200&quot; /&gt;
    &lt;figcaption&gt;Data close to a lower dimensional subspace&lt;/figcaption&gt;
  &lt;/center&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;why-study-ml&quot;&gt;Why study ML?&lt;/h3&gt;

&lt;p&gt;Besides all the buzzwords, machine learning, and indeed artificial intelligence in general, can do some pretty amazing stuff. More than 20 years ago we created AIs that could beat the world’s best chess player, and more recently DeepMind’s AlphaGo beat the world’s best Go player. These are well known examples. As we will learn soon, machine learning provides us with powerful tools to describe data. However, my personal favorite reason for studying machine learning is that it gives us a solid foundation to study more advanced topics such as those typically referred to as deep learning. This umbrella term includes everything from convolutional neural networks to generative adversarial networks to reinforcement learning agents that learn to play hide and seek. Studying ML provides us with mathematical and algorithmic frameworks for analyzing these exciting topics.&lt;/p&gt;

&lt;p&gt;I’ll end with a relatively recent video by OpenAI, where agents learn how to coordinate with other agents to play hide and seek.&lt;/p&gt;

&lt;center&gt;
  &lt;iframe src=&quot;https://www.youtube.com/embed/kopoLzvh5jY&quot; width=&quot;400&quot; height=&quot;300&quot;&gt;&lt;/iframe&gt;
  &lt;figcaption&gt;Credit: OpenAI&lt;/figcaption&gt;
&lt;/center&gt;</content><author><name></name></author><summary type="html">What is Machine Learning?</summary></entry></feed>